# proyecto-SIS313

# üöÄ Proyecto Final SIS313: Sistema de Salas de Espera y Filas Virtuales (Virtual Queue)

> **Asignatura:** SIS313: Infraestructura, Plataformas Tecnol√≥gicas y Redes<br>
> **Semestre:** 2/2025<br>
> **Docente:** Ing. Marcelo Quispe Ortega

## üë• Miembros del Equipo (Grupo Virtual Queue)

| Nombre Completo | Rol en el Proyecto | Contacto (GitHub/Email) |
| :--- | :--- | :--- |
| Duran Chambi Benjamin Ricardo | **Arquitecto de Backend & Proxy:** Encargado de la VM-PROXY y VM-APP (Nginx/Node.js). Dise√±o de la l√≥gica de encolamiento. | Ricardo |
| Escobar Moscoso Jorge Gabriel | **Administrador de Datos:** Encargado de la VM-REDIS. Gesti√≥n de persistencia en memoria y optimizaci√≥n de consultas. | [jogaesmo](https://github.com/jogaesmo) |
| Onofre Alanoca Roy | **Ingeniero de Observabilidad:** Encargado de la VM-MON (Prometheus/Grafana). Monitoreo y auditor√≠a de m√©tricas. | [RoyOnofre](https://github.com/RoyOnofre) |

## üéØ I. Objetivo del Proyecto

> **Objetivo:** Dise√±ar e implementar una arquitectura de **Cola Virtual (Virtual Waiting Room)** distribuida para el sistema universitario SUNiver, capaz de interceptar el 100% del tr√°fico entrante, aplicar **Rate Limiting** para limitar la concurrencia a un umbral seguro (ej. 100 usuarios/minuto) y redirigir el exceso de tr√°fico a una sala de espera est√°tica, garantizando as√≠ la **disponibilidad del servicio cr√≠tico** bajo condiciones de saturaci√≥n.

## üí° II. Justificaci√≥n e Importancia

> **Justificaci√≥n:** El problema recurrente durante las fechas de inscripci√≥n es la **Denegaci√≥n de Servicio Involuntaria** (saturaci√≥n de usuarios), lo que genera una **Falla de Continuidad Operacional (T1)**. Este proyecto es vital porque transforma el fallo en espera, convirtiendo una ca√≠da del servidor (Error 503) en una **espera ordenada**. Implementa una **Protecci√≥n Centralizada (T5)** mediante Nginx para desacoplar la carga masiva y una **Optimizaci√≥n Extrema (T4)** utilizando **Redis (en RAM)** para la gesti√≥n del estado de la cola, resolviendo la concurrencia a latencias de sub-milisegundos.

## üõ†Ô∏è III. Tecnolog√≠as y Conceptos Implementados

### 3.1. Tecnolog√≠as Clave

* **Nginx (VM-PROXY):** Gateway y Reverse Proxy. Protege la topolog√≠a interna y aplica el m√≥dulo `limit_req` para filtrar peticiones abusivas (Rate Limiting).
* **Node.js (Express) (VM-APP):** Servidor de Aplicaci√≥n / L√≥gica. Ejecuta la l√≥gica de "Portero": consulta el estado de la cola a Redis y decide si servir la p√°gina de Login o la Sala de Espera HTML.
* **Redis (VM-REDIS):** Gesti√≥n de Estado Global (Memoria). Base de Datos NoSQL en memoria RAM. Mantiene el contador at√≥mico del aforo en tiempo real, garantizando la "Verdad √önica".
* **Prometheus / Grafana (VM-MON):** Observabilidad y Auditor√≠a. Prometheus hace scraping de m√©tricas de la aplicaci√≥n, y Grafana visualiza la saturaci√≥n de tr√°fico y el comportamiento de la cola.
* **Tailscale / Avahi (mDNS):** Networking Transparente. Proporciona una interconexi√≥n de las VMs mediante nombres de dominio `.local`, facilitando el descubrimiento de servicios sin IPs est√°ticas.

### 3.2. Conceptos de la Asignatura Puestos en Pr√°ctica (T1 - T6)

Marca con un ‚úÖ los temas avanzados de la asignatura que fueron implementados:

* **Alta Disponibilidad (T2) y Tolerancia a Fallos:** ‚úÖ Implementaci√≥n de un patr√≥n **Circuit Breaker L√≥gico** que previene la ca√≠da en cascada del servidor principal mediante el desv√≠o de tr√°fico a la sala de espera est√°tica.
* **Seguridad y Hardening (T5):** ‚úÖ Uso de **Rate Limiting** en el Proxy Inverso (Nginx) para mitigar ataques de fuerza bruta y denegaci√≥n de servicio (DDoS) en Capa 7.
* **Automatizaci√≥n y Gesti√≥n (T6):** ‚úÖ Configuraci√≥n de servicios `systemd` para el arranque autom√°tico y la recuperaci√≥n de servicios (Nginx, Redis, App) tras reinicios no programados.
* **Balanceo de Carga/Proxy (T3/T4):** ‚úÖ Configuraci√≥n de **Upstreams** en Nginx para abstraer la ubicaci√≥n real de los servidores de aplicaci√≥n y permitir escalabilidad horizontal futura.
* **Monitoreo (T4/T1):** ‚úÖ Implementaci√≥n de un stack de observabilidad completo (M√©tricas RED) para detectar cuellos de botella y auditar la estabilidad bajo picos de carga.
* **Networking Avanzado (T3):** ‚úÖ Implementaci√≥n de resoluci√≥n de nombres interna (mDNS) y segmentaci√≥n de servicios en distintas VMs/Hosts.

## üåê IV. Dise√±o de la Infraestructura y Topolog√≠a

### 4.1. Dise√±o Esquem√°tico

El dise√±o se basa en la segmentaci√≥n f√≠sica de los servicios en **4 VMs** distribuidas en hosts distintos, comunicadas por nombre de dominio a trav√©s de la red LAN (mDNS).

> 
| VM/Host | Rol | IP L√≥gica / Hostname | Software Principal | Capa | SO |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **VM-PROXY** | Gateway / Rate Limiter | `proxy-server.local` | Nginx | Acceso | Ubuntu 22.04 |
| **VM-APP** | L√≥gica de Negocio / Worker | `app-server.local` | Node.js v20 | Aplicaci√≥n | Ubuntu 22.04 |
| **VM-REDIS** | Gesti√≥n de Estado (Cola) | `redis-server.local` | Redis Server 7 | Datos | Ubuntu 22.04 |
| **VM-MON** | Monitoreo y Alertas | `monitor-server.local` | Prometheus + Grafana | Gesti√≥n | Ubuntu 22.04 |

### 4.2. Estrategia Adoptada

* **Estrategia de Desacoplamiento:** Se separ√≥ el **Proxy**, la **L√≥gica** y el **Estado** en m√°quinas virtuales distintas. Esto garantiza que la `VM-PROXY` pueda seguir respondiendo con p√°ginas de Sala de Espera, incluso si la `VM-APP` se sobrecarga o falla.
* **Estrategia de Optimizaci√≥n (Redis):** Se eligi√≥ **Redis** sobre una base de datos SQL porque las operaciones de incremento/decremento de cola deben ser at√≥micas y de latencia cero. El uso de la RAM garantiza que el contador de cupos nunca sea el cuello de botella (T4).

## üìã V. Gu√≠a de Implementaci√≥n y Puesta en Marcha

### 5.1. Pre-requisitos (Todas las VMs)

**Objetivo:** Establecer la conectividad por nombre de host (hostname), crucial para que los servicios se encuentren entre s√≠ sin depender de IPs est√°ticas.

1.  **Actualizar e Instalar herramientas b√°sicas y Avahi (mDNS):**
    
    ```bash
    sudo apt update
    sudo apt install -y net-tools curl avahi-daemon libnss-mdns
    ```

2.  **Configurar Hostnames:**
    Ejecutar en cada VM el comando correspondiente:
    * `sudo hostnamectl set-hostname redis-server`
    * `sudo hostnamectl set-hostname app-server`
    * `sudo hostnamectl set-hostname proxy-server`
    * `sudo hostnamectl set-hostname monitor-server`

3.  **Prueba de Conectividad:**
    Desde cualquier VM, verifica la conexi√≥n con otra usando su nombre l√≥gico.
    ```bash
    ping app-server.local
    ```

### 5.2. Despliegue Detallado

#### Paso 1: Configuraci√≥n VM-REDIS (Capa de Datos)

1.  **Instalar Redis Server:**
    ```bash
    sudo apt install redis-server -y
    ```

2.  **Configurar Bind Address:**
    Editar el archivo para permitir conexiones externas desde la VM-APP.
    ```bash
    sudo nano /etc/redis/redis.conf
    ```
    > Buscar la l√≠nea `bind 127.0.0.1` y cambiarla a `bind 0.0.0.0`.

3.  **Reiniciar Servicio:**
    ```bash
    sudo systemctl restart redis-server
    ```

#### Paso 2: Configuraci√≥n VM-APP (L√≥gica del Portero)

1.  **Instalar Node.js y dependencias:**
    ```bash
    sudo apt install nodejs npm -y
    mkdir proyecto && cd proyecto
    npm init -y
    npm install express redis
    ```

2.  **Crear el archivo `index.js`:**
    ```bash
    nano index.js
    ```
    
    Pegar el siguiente c√≥digo de l√≥gica de admisi√≥n:

    ```javascript
    const express = require('express');
    const redis = require('redis');
    const app = express();
    const PORT = 3000;
    const MAX_CAPACITY = 5; 
    const REDIS_KEY = 'suniver:active_users';

    const client = redis.createClient({
        socket: { host: 'redis-server.local', port: 6379 }
    });
    client.connect();

    app.get('/', async (req, res) => {
        try {
            const activeUsers = await client.incr(REDIS_KEY);
            if (activeUsers <= MAX_CAPACITY) {
                setTimeout(() => client.decr(REDIS_KEY), 10000);
                res.send(`<h1>‚úÖ BIENVENIDO (Usuarios: ${activeUsers})</h1>`);
            } else {
                await client.decr(REDIS_KEY);
                res.send(`<h1>üü† SALA DE ESPERA</h1><meta http-equiv="refresh" content="5">`);
            }
        } catch (error) { res.status(503).send('Error'); }
    });

    app.listen(PORT, () => console.log(`App running on ${PORT}`));
    ```

3.  **Ejecutar Servicio:**
    ```bash
    nohup node index.js &
    ```

#### Paso 3: Configuraci√≥n VM-PROXY (Capa de Acceso)

1.  **Instalar Nginx:**
    ```bash
    sudo apt install nginx -y
    ```

2.  **Configurar Rate Limiting:**
    Editar `nginx.conf` para definir la zona de memoria.
    ```bash
    sudo nano /etc/nginx/nginx.conf
    ```
    > Dentro del bloque `http { ... }`, a√±adir:
    > `limit_req_zone $binary_remote_addr zone=one:10m rate=10r/s;`

3.  **Configurar Upstream y Proxy:**
    Editar el archivo por defecto.
    ```bash
    sudo nano /etc/nginx/sites-available/default
    ```
    
    ```nginx
    upstream backend_app {
        server app-server.local:3000;
    }

    server {
        listen 80 default_server;
        location / {
            limit_req zone=one burst=5 nodelay;
            proxy_pass http://backend_app;
            proxy_set_header Host $host;
        }
    }
    ```

4.  **Reiniciar Nginx:**
    ```bash
    sudo nginx -t && sudo systemctl restart nginx
    ```

### 5.3. Ficheros de Configuraci√≥n Clave

* **`index.js` (VM-APP):** Algoritmo de decisi√≥n (If cupo -> Login, Else -> SalaEspera) y conexi√≥n remota a redis-server.local.
* **`nginx.conf` (VM-PROXY):** Define la directiva `limit_req_zone` para el control de acceso.
* **`redis.conf` (VM-REDIS):** Establece `bind 0.0.0.0` para permitir la comunicaci√≥n remota con la VM-APP.
* **`prometheus.yml` (VM-MON):** Configura el scraping (recolecci√≥n) de m√©tricas de la VM-APP.

## ‚ö†Ô∏è VI. Pruebas y Validaci√≥n

| Prueba Realizada | Resultado Esperado | Resultado Obtenido |
| :--- | :--- | :--- |
| **Test de Estr√©s (Saturaci√≥n)** | Al superar el l√≠mite de N usuarios, el usuario N+1 debe ver la pantalla naranja de "Sala de Espera", no el Error 503. | **OK:** El sistema redirigi√≥ correctamente al usuario 6 a la cola. |
| **Test de Recuperaci√≥n Autom√°tica** | Cuando un usuario activo abandona el sistema (timeout), la cola debe avanzar autom√°ticamente. | **OK:** El usuario en espera ingres√≥ autom√°ticamente en el siguiente refresco. |
| **Prueba de Monitoreo en Vivo** | Grafana debe mostrar el pico de tr√°fico y el estancamiento de usuarios activos en el l√≠mite m√°ximo. | **OK:** Dashboard reflej√≥ la curva de saturaci√≥n en tiempo real, validando la estabilidad (T1). |
| **Validaci√≥n de Conectividad** | Las VMs deben comunicarse por hostname (.local) independientemente de la IP asignada por el DHCP. | **OK:** Pings y conexiones de base de datos exitosas por nombre. |

## üìö VII. Conclusiones y Lecciones Aprendidas

**Logro Principal:** Demostramos que la **escalabilidad y la disponibilidad** se logran mediante una arquitectura de software inteligente (**Desacoplamiento**), no solo con la adici√≥n de m√°s hardware. El sistema transform√≥ un escenario de fallo (Error 500) en una experiencia de usuario controlada (Sala de Espera).

**Lecci√≥n Aprendida:** La importancia de **Redis (T4)** es cr√≠tica. Intentar gestionar el estado de la cola en una base de datos tradicional hubiera introducido latencia, convirtiendo al contador en el principal cuello de botella. La gesti√≥n en RAM fue esencial.
